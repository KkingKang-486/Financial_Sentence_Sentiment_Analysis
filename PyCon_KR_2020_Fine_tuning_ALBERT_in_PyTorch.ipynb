{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyCon KR 2020_Fine-tuning ALBERT in PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLdkgBevHo2q"
      },
      "source": [
        "# Fine-tuning KB-ALBERT with `Transformers` PyTorch ver.\n",
        "\n",
        "Hugging Face의 Transformers는 미세조정(fine-tuning)을 위해 필요한 API들을 간단하게 사용할 수 있는 형태로 제공합니다.\n",
        "기초적인 파이썬 프로그래밍과 기계학습 모델의 학습 방법만 알고있다면, 자연어처리(NLP)를 전공하지 않은 일반 사용자도 쉽게 최신 인공지능 언어모델을 불러와 사용할 수 있습니다.\n",
        "\n",
        "그리고 특정 목적을 위해 미세조정된 기계학습 모델을 새로운 데이터에서 쉽게 예측해 볼 수 있도록 Inference Pipeline도 API로 제공하고 있습니다. 그래서 누구나 쉽게 자신이 학습한 모델을 PyTorch나 TensorFlow의 naive programming을 하지 않아도 쉽게 모델을 학습하고 학습된 모델을 배포할 수 있습니다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## Objective\n",
        "- Hugging Face's Transformers를 활용한 fine-tuning\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 네이버 영화리뷰 감성분석 (Naver Movie Review Sentiment Analysis)\n",
        "\n",
        "Naver sentiment movie corpus([link](https://github.com/e9t/nsmc))는 한국어 영화 리뷰 데이터로 리뷰 내용의 긍정과 부정이 라벨링 된 데이터입니다. 15만 개의 학습 데이터와 5만 개의 테스트 데이터로 나누어져 있습니다. 140개 이하의 짧은 문장으로 되어 있고, 쉽게 접근하여 사용해볼 수 있는 데이터입니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. 데이터 준비 & 필요 소스코드 다운로드\n",
        "2. Tokenizer를 활용한 학습용 데이터셋 생성\n",
        "3. 학습 하이퍼파라미터 설정\n",
        "4. Head를 활용한 모델 Fine-tuning\n",
        "5. Inference를 위한 Pipeline 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RECqdMEe5Sqb"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 1. 데이터 및 필요 소스코드 등 준비\n",
        "\n",
        "예제를 수행하기 위해 필요한 데이터와 소스코드를 다운로드 해야 합니다.\n",
        "그리고 아쉽게도 KB-ALBERT는 내부 라이선스 이슈로 Transformers Model Hub에 업로드되어 있지 않아 별도의 과정을 통해 요청하신 후에 다운로드 받아 사용할 수 있습니다. 요청 방법은 [링크](https://github.com/KB-BANK-AI/KB-ALBERT-KO/kb-albert-char)를 참고해주세요.\n",
        "\n",
        "> 참고: 본 예제는 음절단위 모델을 기준으로 작성되었습니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "실행 내용\n",
        "1. Download source code for Custom Tokenizer\n",
        "2. Install Transformers library\n",
        "3. Download NSMC dataset\n",
        "\n",
        "**원활한 모델 학습을 위해 GPU 환경에서 테스트하시기를 권장드립니다.<br> 위 메뉴창에서 \"런타임\" > \"런타임 유형변경\" > \"하드웨어가속기 GPU 선택\" > \"저장\" 으로 환경을 변경하실 수 있습니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7pjxDBDzQyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ff10bf-0b77-44f2-bf14-3b26dc3fd519"
      },
      "source": [
        "# Download source codes\n",
        "!git clone https://github.com/sackoh/pycon-korea-2020-kb-albert.git\n",
        "%cd pycon-korea-2020-kb-albert/\n",
        "\n",
        "# Install transformers\n",
        "%pip install -q transformers\n",
        "\n",
        "# Download NSMC dataset\n",
        "!wget -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pycon-korea-2020-kb-albert'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 41 (delta 19), reused 19 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (41/41), 1.66 MiB | 6.43 MiB/s, done.\n",
            "/content/pycon-korea-2020-kb-albert\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KleHn1j-8ZW8"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Upload pretrained model and configuration files\n",
        "\n",
        "다운로드 받은 파일을 **Google Colab**에서 사용하기 위해서는 파일 업로드가 필요합니다. 아래의 코드를 실행해주세요. 아래를 실행하면 파일 업로드를 위한 팝업창이 열립니다. 다음의 4가지 파일들을 *반드시* 선택하여 업로드를 진행해주세요.\n",
        "\n",
        "- config.json\n",
        "- pytorch_model.bin\n",
        "- tokenizer_config.json\n",
        "- vocab.txt\n",
        "\n",
        "업로드에 다소 시간이 걸릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnobIY4P8tZh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "a604355f-feb1-402a-898e-cf2ddbaf8022"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "%mkdir kb-albert-char\n",
        "%mv config.json pytorch_model.bin tokenizer_config.json vocab.txt ./kb-albert-char"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7506b498-4a35-4fdf-8f79-071b34237d30\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7506b498-4a35-4fdf-8f79-071b34237d30\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving config.json to config.json\n",
            "Saving pytorch_model.bin to pytorch_model.bin\n",
            "Saving tokenizer_config.json to tokenizer_config.json\n",
            "Saving vocab.txt to vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipEtx6AL9jMs"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Load train & test data\n",
        "\n",
        "NSMC 데이터를 불러옵니다. train data와 test data는 각각 리뷰 텍스트와 리뷰의 긍부정 라벨로 구성되어 있습니다.\n",
        "\n",
        "| text | label |\n",
        "| ---  | ---   |\n",
        "| 아 더빙.. 진짜 짜증나네요 목소리 | 0 |\n",
        "| 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 | 1 |\n",
        "| 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ... | 1 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8hFtDRe9Bv_"
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "def read_nsmc_data(file_path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as r:\n",
        "        reader = csv.reader(r, delimiter=\"\\t\")\n",
        "        next(reader, None)\n",
        "        for line in reader:\n",
        "            texts.append(line[1])\n",
        "            labels.append(int(line[2]))\n",
        "    return texts, labels\n",
        "\n",
        "data_dir = Path('./')\n",
        "train_texts, train_labels = read_nsmc_data(data_dir/'ratings_train.txt')\n",
        "test_texts, test_labels = read_nsmc_data(data_dir/'ratings_test.txt')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9hPFzUO-mWK"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 2. Tokenizer를 활용한 학습용 데이터셋 생성\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzV74i5S-3Gk"
      },
      "source": [
        "## Custom Tokenizer 불러오기\n",
        "\n",
        "본 예제는 음절단위(Character-level) 모델을 사용하기 때문에 한국어를 위한 음절단위 Tokenizer를 사용해야 합니다. `Transformers`에서 공식 Tokenizer로 등록이 안됐기 때문에 이전 단계에서 다운로드한 소스코드에서 Custom Tokenizer를 불러옵니다.\n",
        "\n",
        "만약 공식 Hub에 등록된 토크나이저를 사용한다면 다음과 같이 불러오면 됩니다.\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tBSqlVB9S3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00932c26-fa30-44a4-d97b-4f169c08d1df"
      },
      "source": [
        "from tokenization_kbalbert import KbAlbertCharTokenizer\n",
        "\n",
        "tokenizer = KbAlbertCharTokenizer.from_pretrained('kb-albert-char')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'KbAlbertCharTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwM4BcoR_y67"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Tokenizer를 통해 Raw text를 전처리\n",
        "\n",
        "불러온 tokenizer를 활용해 input 데이터를 전처리합니다. 텍스트를 sparse indices 형태로 변환하는 것이 주요 처리 내용입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twl3FoTV9h1A"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suk_8iB6AgSI"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## PyTorch Dataset으로 학습용 데이터셋 생성\n",
        "\n",
        "전처리된 데이터를 파이토치의 `Dataset` 으로 만들어줍니다. `Dataset`과 관련된 자세한 내용은 [링크](https://pytorch.org/docs/stable/data.html)를 참조해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKp4Ktsu-ykZ"
      },
      "source": [
        "import torch\n",
        "\n",
        "class NSMCDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NSMCDataset(train_encodings, train_labels)\n",
        "test_dataset = NSMCDataset(test_encodings, test_labels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Abq-oqxCIN_"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## 3. 학습 하이퍼파라미터 설정\n",
        "\n",
        "`Transformers`에서 제공하는 `TrainingArguments`에 학습 하이퍼파라미터를 설정해줍니다.\n",
        "\n",
        "설정하지 않은 하이퍼파라미터들은 default 값을 사용합니다.\n",
        "\n",
        "하이퍼파라미터 설정을 위한 상세 내용은 [링크](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainingargument#transformers.TrainingArguments)를 참조해주시기 바랍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s28Aw1xPCIka"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2.0,           # 에폭\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=2000\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWddsJNABW0X"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 4. Head를 활용한 모델 Fine-tuning\n",
        "\n",
        "네이버 영화리뷰 감성분석을 위해 텍스트(sequence of words)에서 긍부정(1/0)을 예측하는 모델을 학습합니다. Sequence의 분류문제(Classification) 미세조정을 위한 Head인 `XXXForSequenceClassification`을 불러옵니다. 본 예제는 ALBERT 언어모델을 사용하기 때문에 `AlbertForSequenceClassification` Head class를 불러옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mflrgyl4D2V7"
      },
      "source": [
        "## Fine-tuning을 위한 Head에 사전학습한 언어모델 불러오기\n",
        "\n",
        "Head의 `from_pretrained` 메소드를 통해 사전학습한 KB-ALBERT 언어모델을 함께 불러옵니다. 이때 인자값으로 모델이 위치한 디렉토리(폴더) 경로를 넘겨줍니다.\n",
        "\n",
        "> Head는 Pretrained Language Model과 Output Layer로 구성되어 있습니다.\n",
        "\n",
        "미세조정을 통해 Language Model과 Output Layer의 weight가 함께 조정됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-xh3hzAEifo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f64834-998e-4a93-a74c-d250decab437"
      },
      "source": [
        "from transformers import AlbertForSequenceClassification\n",
        "\n",
        "model = AlbertForSequenceClassification.from_pretrained('kb-albert-char')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at kb-albert-char were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'sop_classifier.classifier.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'sop_classifier.classifier.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at kb-albert-char and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAKdIcvUEofB"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Trainer를 통해 fine-tuning 수행\n",
        "\n",
        "그 다음은 학습을 위해 `Trainer` class를 불러옵니다. Head와 학습 하이퍼파라미터 그리고 앞에서 생성한 학습용 데이터셋을 인자로 넘긴 후에 `train()` 메소드로 학습을 시작합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmXQhwB-Cdhn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c059b74b-7a4d-471e-f3f9-2cb5ca76ddf2"
      },
      "source": [
        "from transformers import Trainer\n",
        "        \n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 150000\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 9376\n",
            "  Number of trainable parameters = 9074818\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9376' max='9376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9376/9376 1:33:42, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.360200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.291100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.232300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.201900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-4500\n",
            "Configuration saved in ./results/checkpoint-4500/config.json\n",
            "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-5000\n",
            "Configuration saved in ./results/checkpoint-5000/config.json\n",
            "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-5500\n",
            "Configuration saved in ./results/checkpoint-5500/config.json\n",
            "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-6000\n",
            "Configuration saved in ./results/checkpoint-6000/config.json\n",
            "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-6500\n",
            "Configuration saved in ./results/checkpoint-6500/config.json\n",
            "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-7000\n",
            "Configuration saved in ./results/checkpoint-7000/config.json\n",
            "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-7500\n",
            "Configuration saved in ./results/checkpoint-7500/config.json\n",
            "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-8000\n",
            "Configuration saved in ./results/checkpoint-8000/config.json\n",
            "Model weights saved in ./results/checkpoint-8000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-8500\n",
            "Configuration saved in ./results/checkpoint-8500/config.json\n",
            "Model weights saved in ./results/checkpoint-8500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-9000\n",
            "Configuration saved in ./results/checkpoint-9000/config.json\n",
            "Model weights saved in ./results/checkpoint-9000/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9376, training_loss=0.25957037238944514, metrics={'train_runtime': 5626.2018, 'train_samples_per_second': 53.322, 'train_steps_per_second': 1.666, 'total_flos': 1792357632000000.0, 'train_loss': 0.25957037238944514, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipbm6tt_DxkH"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## (Optional) Test data에 fine-tuned 모델 성능 평가하기\n",
        "\n",
        "모델 성능 평가를 위한 test dataset과 custom function을 trainer에게 전달하면 평가 결과를 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds98o0oSoRTD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "97ce00bc-7dd6-4d62-f77c-e8e62411d795"
      },
      "source": [
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\n",
        "            'acc': (preds == p.label_ids).mean()\n",
        "        }\n",
        "trainer.compute_metrics = compute_metrics\n",
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 50000\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [782/782 05:59]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.26203206181526184,\n",
              " 'eval_acc': 0.89728,\n",
              " 'eval_runtime': 360.0375,\n",
              " 'eval_samples_per_second': 138.874,\n",
              " 'eval_steps_per_second': 2.172,\n",
              " 'epoch': 2.0}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COL6ZcA7FOvH"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 5. Inference를 위한 Pipeline 생성\n",
        "\n",
        "`Transformers`는 미세조정된 모델에 raw text를 인풋으로 넣었을 때 데이터 전처리와 모델 추론 과정을 `pipeline` class를 통해 api 형태로 쉽게 개발할 수 있도록 하였습니다.\n",
        "\n",
        "pipeline에는 사전 정의된 task 유형과 미세조정된 모델, tokenizer 그리고 딥러닝 프레임워크 유형을 인자로 전달합니다. (PyTorch는 'pt', TensorFlow는 'tf')\n",
        "\n",
        "생성된 pipeline 인스턴스에 raw text를 넣어주면 예측 결과와 예측 결과에 대한 confidence 값이 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQr5iWauMh7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc559f2-f64e-483b-bb93-25adf329c387"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nsmc_classifier = pipeline('sentiment-analysis', \n",
        "                           model=model.cpu(), tokenizer=tokenizer, framework='pt')\n",
        "id2label = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"positive\"}\n",
        "\n",
        "reviews= [\n",
        "          \"하 너무 재밌었다 아이언맨 짱\"\n",
        "]\n",
        "\n",
        "results = nsmc_classifier(reviews)\n",
        "for idx, result in enumerate(results):\n",
        "    print(reviews[idx])\n",
        "    for k, v in result.items():\n",
        "        print(f\" >> {k} : {id2label[v] if k == 'label' else v}\")\n",
        "    print()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "하 너무 재밌었다 아이언맨 짱\n",
            " >> label : positive\n",
            " >> score : 0.9786463379859924\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m6kSkA4NkvzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VGf9zhHnkvwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hzfdPQR5kvt5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGP_twjdHctK"
      },
      "source": [
        "pipeline이나 미세조정된 모델은 `save_pretrained`를 통해 지정된 경로에 저장할 수 있습니다. 다시 사용해야 할 때는 같은 경로명을 `from_pretrained`에 넣어주어 쉽게 불러올 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC2ume9GQOsG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "430efbbc-9bf7-413a-e575-d830e1fc4225"
      },
      "source": [
        "nsmc_classifier.save_pretrained('./pipeline')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./pipeline/config.json\n",
            "Model weights saved in ./pipeline/pytorch_model.bin\n",
            "tokenizer config file saved in ./pipeline/tokenizer_config.json\n",
            "Special tokens file saved in ./pipeline/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-61187adbdde9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnsmc_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pipeline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokenizer_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens_map_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m         save_files = self._save_pretrained(\n\u001b[0m\u001b[1;32m   2172\u001b[0m             \u001b[0msave_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m             \u001b[0mfile_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_save_pretrained\u001b[0;34m(self, save_directory, file_names, legacy_format, filename_prefix)\u001b[0m\n\u001b[1;32m   2213\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"added tokens file saved in {added_tokens_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m         \u001b[0mvocab_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvocab_files\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madded_tokens_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: save_vocabulary() got an unexpected keyword argument 'filename_prefix'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 현재 버전\n",
        "# tokenizers                    0.13.2\n",
        "# transformers                  4.26.0\n",
        "\n",
        "# tensorflow                    2.9.2"
      ],
      "metadata": {
        "id": "OpOCfYBwbf7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VqZflaQbbewC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9pwhnkaaxz7",
        "outputId": "ced97c89-20a5-46e1-b3dd-27025a7443ed"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                       Version\n",
            "----------------------------- ----------------------\n",
            "absl-py                       1.3.0\n",
            "aeppl                         0.0.33\n",
            "aesara                        2.7.9\n",
            "aiohttp                       3.8.3\n",
            "aiosignal                     1.3.1\n",
            "alabaster                     0.7.12\n",
            "albumentations                1.2.1\n",
            "altair                        4.2.0\n",
            "appdirs                       1.4.4\n",
            "arviz                         0.12.1\n",
            "astor                         0.8.1\n",
            "astropy                       4.3.1\n",
            "astunparse                    1.6.3\n",
            "async-timeout                 4.0.2\n",
            "atari-py                      0.2.9\n",
            "atomicwrites                  1.4.1\n",
            "attrs                         22.2.0\n",
            "audioread                     3.0.0\n",
            "autograd                      1.5\n",
            "Babel                         2.11.0\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.6.3\n",
            "bleach                        5.0.1\n",
            "blis                          0.7.9\n",
            "bokeh                         2.3.3\n",
            "branca                        0.6.0\n",
            "bs4                           0.0.1\n",
            "CacheControl                  0.12.11\n",
            "cachetools                    5.2.1\n",
            "catalogue                     2.0.8\n",
            "certifi                       2022.12.7\n",
            "cffi                          1.15.1\n",
            "cftime                        1.6.2\n",
            "chardet                       4.0.0\n",
            "charset-normalizer            2.1.1\n",
            "click                         7.1.2\n",
            "clikit                        0.6.2\n",
            "cloudpickle                   2.2.0\n",
            "cmake                         3.22.6\n",
            "cmdstanpy                     1.0.8\n",
            "colorcet                      3.0.1\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "confection                    0.0.3\n",
            "cons                          0.4.5\n",
            "contextlib2                   0.5.5\n",
            "convertdate                   2.4.0\n",
            "crashtest                     0.3.1\n",
            "crcmod                        1.7\n",
            "cufflinks                     0.17.3\n",
            "cupy-cuda11x                  11.0.0\n",
            "cvxopt                        1.3.0\n",
            "cvxpy                         1.2.3\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.7\n",
            "Cython                        0.29.33\n",
            "daft                          0.0.4\n",
            "dask                          2022.2.1\n",
            "datascience                   0.17.5\n",
            "db-dtypes                     1.0.5\n",
            "dbus-python                   1.2.16\n",
            "debugpy                       1.0.0\n",
            "decorator                     4.4.2\n",
            "defusedxml                    0.7.1\n",
            "descartes                     1.1.0\n",
            "dill                          0.3.6\n",
            "distributed                   2022.2.1\n",
            "dlib                          19.24.0\n",
            "dm-tree                       0.1.8\n",
            "dnspython                     2.2.1\n",
            "docutils                      0.16\n",
            "dopamine-rl                   1.0.5\n",
            "earthengine-api               0.1.335\n",
            "easydict                      1.10\n",
            "ecos                          2.0.12\n",
            "editdistance                  0.5.3\n",
            "en-core-web-sm                3.4.1\n",
            "entrypoints                   0.4\n",
            "ephem                         4.1.4\n",
            "et-xmlfile                    1.1.0\n",
            "etils                         1.0.0\n",
            "etuples                       0.3.8\n",
            "fa2                           0.3.5\n",
            "fastai                        2.7.10\n",
            "fastcore                      1.5.27\n",
            "fastdownload                  0.0.7\n",
            "fastdtw                       0.3.4\n",
            "fastjsonschema                2.16.2\n",
            "fastprogress                  1.0.3\n",
            "fastrlock                     0.8.1\n",
            "feather-format                0.4.1\n",
            "filelock                      3.9.0\n",
            "firebase-admin                5.3.0\n",
            "fix-yahoo-finance             0.0.22\n",
            "Flask                         1.1.4\n",
            "flatbuffers                   1.12\n",
            "folium                        0.12.1.post1\n",
            "frozenlist                    1.3.3\n",
            "fsspec                        2022.11.0\n",
            "future                        0.16.0\n",
            "gast                          0.4.0\n",
            "GDAL                          3.0.4\n",
            "gdown                         4.4.0\n",
            "gensim                        3.6.0\n",
            "geographiclib                 1.52\n",
            "geopy                         1.17.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               2.11.0\n",
            "google-api-python-client      2.70.0\n",
            "google-auth                   2.16.0\n",
            "google-auth-httplib2          0.1.0\n",
            "google-auth-oauthlib          0.4.6\n",
            "google-cloud-bigquery         3.4.1\n",
            "google-cloud-bigquery-storage 2.17.0\n",
            "google-cloud-core             2.3.2\n",
            "google-cloud-datastore        2.11.1\n",
            "google-cloud-firestore        2.7.3\n",
            "google-cloud-language         2.6.1\n",
            "google-cloud-storage          2.7.0\n",
            "google-cloud-translate        3.8.4\n",
            "google-colab                  1.0.0\n",
            "google-crc32c                 1.5.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        2.4.0\n",
            "googleapis-common-protos      1.58.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.10.1\n",
            "greenlet                      2.0.1\n",
            "grpcio                        1.51.1\n",
            "grpcio-status                 1.48.2\n",
            "gspread                       3.4.2\n",
            "gspread-dataframe             3.0.8\n",
            "gym                           0.25.2\n",
            "gym-notices                   0.0.8\n",
            "h5py                          3.1.0\n",
            "HeapDict                      1.0.1\n",
            "hijri-converter               2.2.4\n",
            "holidays                      0.18\n",
            "holoviews                     1.14.9\n",
            "html5lib                      1.0.1\n",
            "httpimport                    0.5.18\n",
            "httplib2                      0.17.4\n",
            "httpstan                      4.6.1\n",
            "huggingface-hub               0.12.0\n",
            "humanize                      0.5.1\n",
            "hyperopt                      0.1.2\n",
            "idna                          2.10\n",
            "imageio                       2.9.0\n",
            "imagesize                     1.4.1\n",
            "imbalanced-learn              0.8.1\n",
            "imblearn                      0.0\n",
            "imgaug                        0.4.0\n",
            "importlib-metadata            6.0.0\n",
            "importlib-resources           5.10.2\n",
            "imutils                       0.5.4\n",
            "inflect                       2.1.0\n",
            "intel-openmp                  2023.0.0\n",
            "intervaltree                  2.1.0\n",
            "ipykernel                     5.3.4\n",
            "ipython                       7.9.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.3.9\n",
            "ipywidgets                    7.7.1\n",
            "itsdangerous                  1.1.0\n",
            "jax                           0.3.25\n",
            "jaxlib                        0.3.25+cuda11.cudnn805\n",
            "jieba                         0.42.1\n",
            "Jinja2                        2.11.3\n",
            "joblib                        1.2.0\n",
            "jpeg4py                       0.1.4\n",
            "jsonschema                    4.3.3\n",
            "jupyter-client                6.1.12\n",
            "jupyter-console               6.1.0\n",
            "jupyter_core                  5.1.3\n",
            "jupyterlab-widgets            3.0.5\n",
            "kaggle                        1.5.12\n",
            "kapre                         0.3.7\n",
            "keras                         2.9.0\n",
            "Keras-Preprocessing           1.1.2\n",
            "keras-vis                     0.4.1\n",
            "kiwisolver                    1.4.4\n",
            "korean-lunar-calendar         0.3.1\n",
            "langcodes                     3.3.0\n",
            "libclang                      15.0.6.1\n",
            "librosa                       0.8.1\n",
            "lightgbm                      2.2.3\n",
            "llvmlite                      0.39.1\n",
            "lmdb                          0.99\n",
            "locket                        1.0.0\n",
            "logical-unification           0.4.5\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.9.2\n",
            "Markdown                      3.4.1\n",
            "MarkupSafe                    2.0.1\n",
            "marshmallow                   3.19.0\n",
            "matplotlib                    3.2.2\n",
            "matplotlib-venn               0.11.7\n",
            "miniKanren                    1.0.3\n",
            "missingno                     0.5.1\n",
            "mistune                       0.8.4\n",
            "mizani                        0.7.3\n",
            "mkl                           2019.0\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                9.0.0\n",
            "moviepy                       0.2.3.5\n",
            "mpmath                        1.2.1\n",
            "msgpack                       1.0.4\n",
            "multidict                     6.0.4\n",
            "multipledispatch              0.6.0\n",
            "multitasking                  0.0.11\n",
            "murmurhash                    1.0.9\n",
            "music21                       5.5.0\n",
            "natsort                       5.5.0\n",
            "nbconvert                     5.6.1\n",
            "nbformat                      5.7.1\n",
            "netCDF4                       1.6.2\n",
            "networkx                      3.0\n",
            "nibabel                       3.0.2\n",
            "nltk                          3.7\n",
            "notebook                      5.7.16\n",
            "numba                         0.56.4\n",
            "numexpr                       2.8.4\n",
            "numpy                         1.21.6\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.2.2\n",
            "okgrade                       0.4.3\n",
            "opencv-contrib-python         4.6.0.66\n",
            "opencv-python                 4.6.0.66\n",
            "opencv-python-headless        4.7.0.68\n",
            "openpyxl                      3.0.10\n",
            "opt-einsum                    3.3.0\n",
            "osqp                          0.6.2.post0\n",
            "packaging                     21.3\n",
            "palettable                    3.3.0\n",
            "pandas                        1.3.5\n",
            "pandas-datareader             0.9.0\n",
            "pandas-gbq                    0.17.9\n",
            "pandas-profiling              1.4.1\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.12.1\n",
            "param                         1.12.3\n",
            "parso                         0.8.3\n",
            "partd                         1.3.0\n",
            "pastel                        0.2.1\n",
            "pathlib                       1.0.1\n",
            "pathy                         0.10.1\n",
            "patsy                         0.5.3\n",
            "pep517                        0.13.0\n",
            "pexpect                       4.8.0\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        7.1.2\n",
            "pip                           22.0.4\n",
            "pip-tools                     6.6.2\n",
            "platformdirs                  2.6.2\n",
            "plotly                        5.5.0\n",
            "plotnine                      0.8.0\n",
            "pluggy                        0.7.1\n",
            "pooch                         1.6.0\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.3\n",
            "preshed                       3.0.8\n",
            "prettytable                   3.6.0\n",
            "progressbar2                  3.38.0\n",
            "prometheus-client             0.15.0\n",
            "promise                       2.3\n",
            "prompt-toolkit                2.0.10\n",
            "prophet                       1.1.1\n",
            "proto-plus                    1.22.2\n",
            "protobuf                      3.19.6\n",
            "psutil                        5.4.8\n",
            "psycopg2                      2.9.5\n",
            "ptyprocess                    0.7.0\n",
            "py                            1.11.0\n",
            "pyarrow                       9.0.0\n",
            "pyasn1                        0.4.8\n",
            "pyasn1-modules                0.2.8\n",
            "pycocotools                   2.0.6\n",
            "pycparser                     2.21\n",
            "pyct                          0.4.8\n",
            "pydantic                      1.10.4\n",
            "pydata-google-auth            1.5.0\n",
            "pydot                         1.3.0\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyemd                         0.5.1\n",
            "pyerfa                        2.0.0.1\n",
            "Pygments                      2.6.1\n",
            "PyGObject                     3.36.0\n",
            "pylev                         1.4.0\n",
            "pymc                          4.1.4\n",
            "PyMeeus                       0.5.12\n",
            "pymongo                       4.3.3\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.6\n",
            "pyparsing                     3.0.9\n",
            "pyrsistent                    0.19.3\n",
            "pysimdjson                    3.2.0\n",
            "PySocks                       1.7.1\n",
            "pystan                        3.3.0\n",
            "pytest                        3.6.4\n",
            "python-apt                    2.0.1\n",
            "python-dateutil               2.8.2\n",
            "python-louvain                0.16\n",
            "python-slugify                7.0.0\n",
            "python-utils                  3.4.5\n",
            "pytz                          2022.7\n",
            "pyviz-comms                   2.2.1\n",
            "PyWavelets                    1.4.1\n",
            "PyYAML                        6.0\n",
            "pyzmq                         23.2.1\n",
            "qdldl                         0.1.5.post2\n",
            "qudida                        0.0.4\n",
            "regex                         2022.6.2\n",
            "requests                      2.25.1\n",
            "requests-oauthlib             1.3.1\n",
            "requests-unixsocket           0.2.0\n",
            "resampy                       0.4.2\n",
            "rpy2                          3.5.5\n",
            "rsa                           4.9\n",
            "scikit-image                  0.18.3\n",
            "scikit-learn                  1.0.2\n",
            "scipy                         1.7.3\n",
            "screen-resolution-extra       0.0.0\n",
            "scs                           3.2.2\n",
            "seaborn                       0.11.2\n",
            "Send2Trash                    1.8.0\n",
            "setuptools                    57.4.0\n",
            "setuptools-git                1.2\n",
            "shapely                       2.0.0\n",
            "six                           1.15.0\n",
            "sklearn-pandas                1.8.0\n",
            "smart-open                    6.3.0\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "soundfile                     0.11.0\n",
            "spacy                         3.4.4\n",
            "spacy-legacy                  3.0.11\n",
            "spacy-loggers                 1.0.4\n",
            "Sphinx                        3.5.4\n",
            "sphinxcontrib.applehelp       1.0.3\n",
            "sphinxcontrib-devhelp         1.0.2\n",
            "sphinxcontrib-htmlhelp        2.0.0\n",
            "sphinxcontrib-jsmath          1.0.1\n",
            "sphinxcontrib-qthelp          1.0.3\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "SQLAlchemy                    1.4.46\n",
            "sqlparse                      0.4.3\n",
            "srsly                         2.4.5\n",
            "statsmodels                   0.12.2\n",
            "sympy                         1.7.1\n",
            "tables                        3.7.0\n",
            "tabulate                      0.8.10\n",
            "tblib                         1.7.0\n",
            "tenacity                      8.1.0\n",
            "tensorboard                   2.9.1\n",
            "tensorboard-data-server       0.6.1\n",
            "tensorboard-plugin-wit        1.8.1\n",
            "tensorflow                    2.9.2\n",
            "tensorflow-datasets           4.8.1\n",
            "tensorflow-estimator          2.9.0\n",
            "tensorflow-gcs-config         2.9.1\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.29.0\n",
            "tensorflow-metadata           1.12.0\n",
            "tensorflow-probability        0.17.0\n",
            "termcolor                     2.2.0\n",
            "terminado                     0.13.3\n",
            "testpath                      0.6.0\n",
            "text-unidecode                1.3\n",
            "textblob                      0.15.3\n",
            "thinc                         8.1.6\n",
            "threadpoolctl                 3.1.0\n",
            "tifffile                      2022.10.10\n",
            "tokenizers                    0.13.2\n",
            "toml                          0.10.2\n",
            "tomli                         2.0.1\n",
            "toolz                         0.12.0\n",
            "torch                         1.13.1+cu116\n",
            "torchaudio                    0.13.1+cu116\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.14.1\n",
            "torchvision                   0.14.1+cu116\n",
            "tornado                       6.0.4\n",
            "tqdm                          4.64.1\n",
            "traitlets                     5.7.1\n",
            "transformers                  4.26.0\n",
            "tweepy                        3.10.0\n",
            "typeguard                     2.7.1\n",
            "typer                         0.7.0\n",
            "typing_extensions             4.4.0\n",
            "tzlocal                       1.5.1\n",
            "uritemplate                   4.1.1\n",
            "urllib3                       1.24.3\n",
            "vega-datasets                 0.9.0\n",
            "wasabi                        0.10.1\n",
            "wcwidth                       0.2.5\n",
            "webargs                       8.2.0\n",
            "webencodings                  0.5.1\n",
            "Werkzeug                      1.0.1\n",
            "wheel                         0.38.4\n",
            "widgetsnbextension            3.6.1\n",
            "wordcloud                     1.8.2.2\n",
            "wrapt                         1.14.1\n",
            "xarray                        2022.12.0\n",
            "xarray-einstats               0.4.0\n",
            "xgboost                       0.90\n",
            "xkit                          0.0.0\n",
            "xlrd                          1.2.0\n",
            "xlwt                          1.3.0\n",
            "yarl                          1.8.2\n",
            "yellowbrick                   1.5\n",
            "zict                          2.2.0\n",
            "zipp                          3.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kl3XRSP5bRYp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}